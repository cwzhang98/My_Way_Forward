# Generative Adversarial Network

## Theory

### purpose(Generator)

- N:normal distribution(can be other distribution)
- G:generator
- $P_G$:distribution generated by G
- $P_{data}$:real distribution

Objective of GAN:
$$
N\rightarrow G\rightarrow P_G
$$

> Make $P_G$ as close as possible with $P_{data}$. i.e. minimize $Div(P_G,P_{data})$

To find our target $G$,as $G^*$
$$
G^*=\mathop{\arg\min}_G \mathit{Div}(P_G,P_{data})
$$

### how to?(Discriminator)

> Although we don't know the distribution of $P_G$ and $P_{data}$(Which means wo can't calculate $Div$),we can sample fro  m them.

- D:discriminator

Discriminator will distinguish and evaluate the samples which form $P_{data}$ or $P_G$,samples form $P_{data}$ will increase the score through $V(G,D)$, and $P_G$ will decrease it.

To find our target $D$,as $D^*$

Training:
$$
D^*=\mathop{\arg\max}_D V(D,G)
$$
Objective function(loss function) for $D$:
$$
V(G,D)=E_{y\sim P_{data}}[logD(y)]+E_{y\sim P_{G}}[log(1-D(y))]
$$

> The maximum objective value is related to JS divergence,which means equation (3) is related to equation (2)
>
> Sow can get a new equation for $G^*$

$$
G^*=\arg\min_G\max_D V(G,D)
$$

## WGAN

> Instead of using $Div(P_G,P_{data})$, Evaluate Wasserstein distance between $P_{data}$ and $P_G$

$$
\max_{D\in 1-Lipschitz} \{ E_{y\sim P_{data}}[D(y)]-E_{y\sim P_G}D(y) \}
$$

### how to find a D belongs to 1-Lipschitz ?

- Improved WGAN (Gradient Penalty) 1704.00028
- Spectral Normalization (Keep gradient norm) 1802.05957

## Tips for training GAN

- 1511.06434
- 1606.03498
- 1809.11096

## GAN for Sequence Generation

- 1905.09922
